
This is an educational Transformers training library. I created this to understand the core mechanics of transformers, implementing everything in pure pytorch (other than distributed training of course)

Use cases include refreshing your memory about transformers, interview prep, matrix-mechanics, etc. 


## Checklist

### Transformer Architecture

[x] GPT-2 <br>
[x] Llama <br>

[x] MoE <br>

### Training Optimizations

[x] Flash Attention <br>
[x] Gradient Accumulation <br>
[x] Mixed Precision Training - torch.autocast <br>
[x] GMQA / MQA <br>


### Distributed Training

[x] DDP (Data Parallel) <br>
[x] FSDP (Fully sharded data parallel) <br>
[ ] Model Parallel <br>
[ ] Pipeline Parallel <br>
  