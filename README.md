
This is am educational Transformers training library. I created this to understand the core mechanics of transformers, implementing everything in pure pytorch (other than distributed training)

I use this to refresh my memory about transformers, as well as interview prep. 


## Checklist

### Transformer Architecture

[x] GPT-2
[x] Llama

[x] MoE 

### Training Optimizations

[x] Flash Attention
[x] Gradient Accumulation
[x] Mixed Precision Training - torch.autocast
[x] GMQA / MQA


### Distributed Training

[x] DDP (Data Parallel) <br>
[x] FSDP (Fully sharded data parallel) <br>
[ ] Model Parallel <br>
[ ] Pipeline Parallel <br>
  